---
title: |
  | \vspace{5cm} \LARGE Predicting Used Car Prices
  | \vspace{0.5cm} \Large ECON 573 Group 6
author: "Jemin Park, Jonah Selom, Keegan McDowell"
date: "4/30/2022"
# setspace: doublespacing
fontsize: 12pt
documentclass: article
geometry: margin=3cm
indent: true
output:
  pdf_document:
    highlight: tango
    number_sections: true
header-includes:
  - \usepackage{setspace}\doublespacing
  - \setlength{\parindent}{4em}
  - \setlength{\parskip}{0em}

---

```{r setup, include=FALSE}
# don't want to show code on default
knitr::opts_chunk$set(echo = FALSE) 
rm(list=ls())

library(tidyverse) # cleaning
library(ggplot2)

library(MASS)
library(leaps) # selection
library(glmnet) # lasso
library(gbm) # boosting
library(randomForest) # bagging

```

\newpage
\maketitle
\newpage
\tableofcontents
\newpage


# Introduction 

Filler paragraph 1

text

Filler para 2

# Data

The data used in this paper was found on Kaggle posted by a user who had originally taken the data from a larger set that included separate datasets for each individual car manufacturer. The original dataset was scraped from thousands as used car listings and was used to predict the selling price of a car that a friend of the creator was debating whether or not to sell. The creator then reshaped the product in order to generalize it more as a prediction model for used cars selling in the UK. The variables used, in this case in an attempt to predict the price of the vehicle, are transmission, mileage, fuel type, tax, miles per gallon, and engine size. The user who transformed the dataset into the one used in this paper was able to clean the data a bit more thoroughly, accounting for any issues made during the original creators scraping process so that the data would then be easier to use. They also changed the breakdown of the data, combining each of the manufacturers and instead split them into predetermined test and training sets, eliminating the randomness of that result.


## Data Preprocessing

The data originally was divided into sections of x_test, x_train, y_test, and y_train. Knowing that some models would need the more complete dataset in order to run, the train data and the test data were both merged by their common column, which was the carID variable, being used as a common identifier between the split data. After that, there were two variables, transmission, showing whether the car was automatic, manual, or somewhere in between, and fuelType, including both types of gasoline, but also if the car is electric or a hybrid, that could be more useful as factor variables. It was also necessary to examine whether the data had a large amount of missing data, but it was relatively clean when taken from the source, so this was not a major issue. With these barriers out of the way, the data became easier to use through the regression methods used throughout the rest of the paper. 

```{r data-import}
# read in all data, with character cols as factors
x_test = read.csv("X_test.csv", stringsAsFactors=TRUE) 
x_train = read.csv("X_train.csv", stringsAsFactors=TRUE)
y_test = read.csv("y_test.csv", stringsAsFactors=TRUE)
y_train = read.csv("y_train.csv", stringsAsFactors=TRUE)

# join and remove carID
test = merge(x_test, y_test, by.x = "carID")[,-1] 
train = merge(x_train, y_train, by.x = "carID")[,-1]

# summary(train)

# length(unique(train$brand))
# summary(train$brand)

```

A potential issue with the data would come in trying to run the regression with the car models attribute involved. With 90 different car models, compared to having nearly 7500 total observations, the flexibility of the model becomes a question. Including the variable could lead the model to be too flexible, especially with a low number of observations (suprisingly, a Toyota Camry, one of the most popular used cars, has only 8 observations) causing it to follow any noise too closely and increases variance. A high flexibility can be good when looking solely for prediction, but can cause issues if it is controlling the results of the model too greatly. Collinearity could also be an issue between factors such as mileage and miles per gallon which are likely to decrease/increase as the years go on, but this should be less of an issue because, although they are related in that way, they will still be affecting the price of the car independently.

## Data Visualization




(each into separate sections as needed)


# Methods

We are interested in building predictive models that accurately determine price based on the aforementioned covariates. While there exist the issues above, we believe for the purposes of this paper and for simplicity, we want to build strong, simple models that can best infer car prices. To do this, we will need to consider outcomes in which we can sufficiently compare test outcomes. 

Fortunately, data was split beforehand (as part of a competition dataset), with 4960 observations in the training set, and 2672 in the test set. One key element of interest would be to observe how estimated test errors (in our case, we will use cross-validation error), and how they place may differ from modeled outcomes when calculating test error on the actual test set. For this paper, we will focus on the implementation and predictive ability of 5 key models: linear regression, LASSO regression, ridge regression, boosting, and random forests. 


## Linear Regression

```{r slr, include = FALSE}
mod.slr = lm(price ~ .  - model , data = train)
summary(mod.slr)

train.lprice = log(train$price)
mod.glm = lm(train.lprice ~ .  - model - price , data = train)
summary(mod.glm)


# training MSE
mean(mod.slr$residuals^2)
mean(mod.glm$residuals^2)

# # test mse
# test.lprice = log(test$price)
# slr.pred = predict(mod.slr, x_test)
# mean((slr.pred - test$price)^2)

par(mfrow=c(2,2))
plot(mod.slr)
par(mfrow=c(2,2))
plot(mod.glm)

```

We started building a simple multilinear model by regressing all predictive covariates *except* the car model on price. We first excluded `model` to give us an interpretable predictive model, and we found that nearly all the predictors except `fuelTypeElectric`, `fuelTypeOther`, and `transmissionOther` were significant at $0.001\%$ (with the “other” fuel type being significant at $0.01\%$), with an impressive $R^2$ of $0.7194$. We note that every brand listed has some significant predictability on their respective effects on price. However, upon examining diagnostic data, we see that the standardized residuals indicate irregularity in the upper quantiles. 

So, we then tested a base model on the log of car price, we found that this addressed the issue of regularity in the model, while simultaneously increasing the significance of the predictors. This had improved the $R^2$ to $0.884$ on the base model that we were testing. Thus, continuing forward, we will take log price into full consideration for all following models. Below is the summary output for the log model on all predictors but the car model:

```{r}
summary(mod.glm)
```

Next, we want



## LASSO Regression


```{r}
#lasso train
xt = data.matrix(x_train[, c('carID', 'year', 'transmission', 'mileage', 'fuelType', 'tax', 'mpg', 'engineSize')])
yt = y_train$price

cv.lasso = cv.glmnet(xt, yt, alpha = 1)

best_lambda = cv.lasso$lambda.min
best_lambda

plot(cv.lasso)
coef(cv.lasso)

sqrt(cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se])

#Best model
best_model = glmnet(xt, yt, alpha = 1, lambda = best_lambda)
coef(best_model)

y_predicted = predict(best_model, s = best_lambda, newx = xt)

#SST and SSE
sst = sum((yt - mean(yt))^2)
sse = sum((y_predicted - yt)^2)

#R-squared
rsq = 1 - sse/sst
rsq

#lasso test
x = data.matrix(x_test[, c('carID', 'year', 'transmission', 'mileage', 'fuelType', 'tax', 'mpg', 'engineSize')])
y = y_test$price

cv.lasso = cv.glmnet(x, y, alpha = 1)

best_lambda = cv.lasso$lambda.min
best_lambda

plot(cv.lasso)
coef(cv.lasso)

sqrt(cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se])

#Best model
best_model = glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)

y_predicted = predict(best_model, s = best_lambda, newx = x)


#SST and SSE
sst = sum((y - mean(y))^2)
sse = sum((y_predicted - y)^2)

#R-squared
rsq = 1 - sse/sst
rsq
```
The LASSO regression stands for the Least Absolute Shrinkage and Selection Operator. The goal of the Lasso regression is to obtain the subset of predictors that minimizes prediction error for a response variable. The Lasso regression does this by causing some of the coefficients to shrink to zero to create a more efficient model. With alpha set at 1 and using regular price on the training set, the best lambda was found to be 32.0433. The train R Square for the lasso regression was 0.6674 with the RMSE being 9722.162. The test R square for lasso regression was found to be 0.6528 with the RMSE being 10315.55. In the training data set, the model explains 66.74% of variation in the response values of the training data. In the test data set, the model explains 65.28% of the variance in response values. When accounting for the difference in the R-squared of the training and test data sets, it makes sense that training R-squared is higher than test R-squared because the training set has a greater share of the data and accounts for more variance in price. 
The Lasso regression puts emphasis on year, mileage, and engine size while bringing all other variables to 0. Year is an obvious variable to include into the model as newer cars often have more features, less wear, and are more desirable. Mileage is considered an important variable to include in the model because it is highly influential in determining the price of a used car as generally the higher mileage a car has the lower its price will be. Engine size is the final variable included in the model which indicates that cars with larger engines tend to cost more as they are more expensive to maintain and are found in new vehicles with higher value which translates to the used market. 	


## Ridge Regression

Discussion & results


```{r}
#Ridge train
xt = data.matrix(x_train[, c('carID', 'year', 'transmission', 'mileage', 'fuelType', 'tax', 'mpg', 'engineSize')])
yt = y_train$price

cv.ridge = cv.glmnet(xt, yt, alpha = 0)

best_lambda = cv.ridge$lambda.min
best_lambda

plot(cv.ridge)
coef(cv.ridge)

sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])

#Best model
best_model = glmnet(xt, yt, alpha = 0, lambda = best_lambda)
coef(best_model)

y_predicted = predict(best_model, s = best_lambda, newx = xt)


#SST and SSE
sst = sum((yt - mean(yt))^2)
sse = sum((y_predicted - yt)^2)

#R-squared
rsq = 1 - sse/sst
rsq


#Ridge test
x = data.matrix(x_test[, c('carID', 'year', 'transmission', 'mileage', 'fuelType', 'tax', 'mpg', 'engineSize')])
y = y_test$price

cv.ridge = cv.glmnet(x, y, alpha = 0)

best_lambda = cv.ridge$lambda.min
best_lambda

plot(cv.ridge)
coef(cv.ridge)

sqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])

#Best model
best_model = glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(best_model)

y_predicted = predict(best_model, s = best_lambda, newx = x)


#SST and SSE
sst = sum((y - mean(y))^2)
sse = sum((y_predicted - y)^2)

#R-squared
rsq = 1 - sse/sst
rsq
```
The Ridge regression is used to analyze data that suffers from multicollinearity. RIdge regression addresses this problem by shrinking the coefficients to near zero which helps to prevent overfitting. With alpha set to 0, the best lambda was found to be 1025.15 for the training data set with a train R-squared of 0.6648 and an RMSE of 9707.688. THe test R-squared was 0.6505 with an RMSE of 10243.94. Similar to the Lasso regression, the test R-squared is higher than the train R-squared as it covers more of the data. In addition, variables like year and mileage that were included in the lasso regression were given importance in the ridge regression. Unlike the lasso, transmission and fuel type were given greater importance in ridge regression compared to lasso. The variables chosen in the ridge regression appear to be more inclusive of the data as a whole when considering that manual cars are often cheaper than their automatic counterparts and that the price of diesel is generally more expensive than gasoline which means that the price of diesel vehicles are generally higher.
## Model 4

```{r}
# bagging (recall m = p)
bag.price = randomForest(y_train$price~., data=x_train, mtry=ncol(x_train)-1, importance=TRUE)
bag.pred = predict(bag.price, newdata=x_test)

# bagging test MSE
mean((y_test$price - bag.pred)^2)

#Chart
plot(y_test$price, bag.pred)

#Correlation
cor(y_test$price, bag.pred)
```
Bagging (Random Forests) is a method which uses decision trees where it raises the overall stability of a model by improving accuracy, reducing variance thereby eliminating the overfitting of data. Running bagging on the test data resulted in a model that was highly predictive of the car’s price.
The resulting chart had a 0.9695 correlation between the random forest model and the test set. The bagging Test MSE was 16715493, which is unusually high. Overall, the random forest model was extremely predictive at predicting used car prices.


## Model 5

```{r}
#Boosting
library(gbm)
set.seed(1)

y_train$price = log(y_train$price)
y_test$price = log(y_test$price)

lambdas = 10 ^ seq(-10, 0, 0.1) # 100 lambdas
log.lambdas = log10(lambdas)

train.mse = rep(NA, length(lambdas))
test.mse = rep(NA, length(lambdas))

set.seed(1)
for (i in 1:length(lambdas)) {
  boost.price = gbm(y_train$price ~ ., data = (x_test[, c('year', 'transmission', 'mileage', 'fuelType', 'tax', 'mpg', 'engineSize')]), distribution="gaussian", n.trees=1000, shrinkage=lambdas[i])
  
  train.pred = predict(boost.price, newdata=x_train, n.trees=1000)
  train.mse[i] = mean((train.pred - y_train$price)^2)
  
  test.pred = predict(boost.price, newdata=x_test, n.trees=1000)
  test.mse[i] = mean((test.pred - y_test$price)^2)
}


par(mfrow=c(1, 2))
plot(lambdas,train.mse,type="b",xlab=expression(shrinkage), ylab="Train MSE", col="red", pch=20)
plot(log.lambdas,train.mse,type="b",xlab=expression(shrinkage), ylab="Train MSE", col="red", pch=20)


# Minumum training lambda
lambdas[which.min(train.mse)]

# Minimum training MSE
min(train.mse)

# Minumum test lambda
boost.min.lambda = lambdas[which.min(test.mse)]
boost.min.lambda

# Minimum test MSE
min(test.mse)
```
Boosting is a process in which trees are grown and subsequent trees are created using the information of the previous trees to minimize training error. The boosting model employs 1000 trees to predict the prices of a used vehicle from all of the predictor variables. The optimal training lambda is essentially 0 (1e-10), while the training MSE is 0.4265. For the test set, the optimal lambda is 0.00398, while the test MSE is 0.4798. 



Discussion & results

# Conclusion

Final discussion with interpretation from model, inferences, best results, etc.

